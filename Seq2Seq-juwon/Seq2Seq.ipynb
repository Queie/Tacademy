{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "# **Seq 2 Seq**\n",
    "시퀀스 2 시퀀스 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **1 Image Array to matplotlib**\n",
    "이미지 <--> numpy Array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "image =[[1,1,1,1,1,1,1,1,1,1,1,1,0.1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,0.2,1,1,1,0.3,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,0.1,1,1,1,0.3,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "       [1,1,1,1,1,1,1,0,1,1,1,2,1,1],\n",
    "       [1,1,1,1,1,1,1,1,1,1,1,1,1,1]]\n",
    "dx, dy = 0.05, 0.05\n",
    "x ,  y = np.arange(-3.0, 3.0, dx), np.arange(-3.0, 3.0, dy)\n",
    "X ,  Y = np.meshgrid(x, y)\n",
    "xmin, xmax, ymin, ymax = np.amin(x), np.amax(x), np.amin(y), np.amax(y)\n",
    "extent = xmin, xmax, ymin, ymax\n",
    "image  = np.array(image); print(image.shape)\n",
    "\n",
    "plt.figure(frameon=False)\n",
    "plt.imshow(image, cmap=plt.cm.gray, interpolation='nearest', extent=extent)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **2 번역**\n",
    "http://dalpo0814.tistory.com/45\n",
    "\n",
    "LSTM 은 긴 번역에 대해선 성능이 좋지 않은것에 대한 대안으로써 보완한 모델이다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/erdos/Jupyter/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from imageio import imread, imsave\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "img  = imread('./data/코끼리_tinted.jpg')\n",
    "img1 = imread('./data/햄스터_tinted.jpg')\n",
    "img2 = imread('./data/코끼리11.jpg')\n",
    "img3 = imread('./data/코끼리22.jpg')\n",
    "img4 = imread('./data/코끼리33.jpg')\n",
    "img5 = imread('./data/코끼리44.jpg')\n",
    "data = [img, img1,img2,img3,img4]\n",
    "data = np.array(data)\n",
    "char_arr = [c for c in 'SEP가아나기다코구끼라리와마오리바햄스사터뒤에물통 ']\n",
    "num_dic  = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len  = len(num_dic)+1\n",
    "max_len  = 10 # 입력과 출력의 최대 글자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inputs, filters, kernel_size = (3, 3), strides = (1, 1),\n",
    "           activation = tf.nn.relu, use_bias = True, name = None):\n",
    "\n",
    "    return tf.layers.conv2d(inputs = inputs, filters = filters,\n",
    "                            kernel_size = kernel_size, strides = strides,\n",
    "                            padding = 'same', activation = activation,\n",
    "                            use_bias = use_bias, name = name)\n",
    "\n",
    "# 2D Max Pooling layer\n",
    "def max_pool2d(inputs, pool_size=(2, 2), strides=(2, 2), name = None):\n",
    "    return tf.layers.max_pooling2d(inputs = inputs, pool_size = pool_size,\n",
    "                                   strides = strides, padding = 'same', name = name)\n",
    "# Fully-connected layer\n",
    "def dense(inputs, units, activation = tf.tanh, use_bias = True, name = None):\n",
    "    return tf.layers.dense(inputs = inputs, units = units, activation = activation,\n",
    "                           use_bias = use_bias, name = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VGG16 net\n",
    "def build_vgg16(images):\n",
    "    conv1_1_feats = conv2d(images,        64,  name='conv1_1')\n",
    "    conv1_2_feats = conv2d(conv1_1_feats, 64,  name='conv1_2')\n",
    "    pool1_feats   = max_pool2d(conv1_2_feats,  name='pool1')\n",
    "    \n",
    "    conv2_1_feats = conv2d(pool1_feats,   128, name='conv2_1')\n",
    "    conv2_2_feats = conv2d(conv2_1_feats, 128, name='conv2_2')\n",
    "    pool2_feats   = max_pool2d(conv2_2_feats,  name='pool2')\n",
    "    \n",
    "    conv3_1_feats = conv2d(pool2_feats,   256, name='conv3_1')\n",
    "    conv3_2_feats = conv2d(conv3_1_feats, 256, name='conv3_2')\n",
    "    conv3_3_feats = conv2d(conv3_2_feats, 256, name='conv3_3')\n",
    "    pool3_feats   = max_pool2d(conv3_3_feats,  name='pool3')\n",
    "    \n",
    "    conv4_1_feats = conv2d(pool3_feats,   256, name='conv4_1')\n",
    "    conv4_2_feats = conv2d(conv4_1_feats, 256, name='conv4_2')\n",
    "    conv4_3_feats = conv2d(conv4_2_feats, 256, name='conv4_3')\n",
    "    pool4_feats   = max_pool2d(conv4_3_feats,  name='pool4')\n",
    "    \n",
    "    conv5_1_feats = conv2d(pool4_feats,   256, name='conv5_1')\n",
    "    conv5_2_feats = conv2d(conv5_1_feats, 256, name='conv5_2')\n",
    "    conv5_3_feats = conv2d(conv5_2_feats, 256, name='conv5_3') # 14*14*256 로 출력\n",
    "    \n",
    "    reshaped_conv5_3_feats = tf.reshape(conv5_3_feats, [-1, 196*256] )\n",
    "    den = dense(reshaped_conv5_3_feats,9192)\n",
    "    den = dense(den, 4096)\n",
    "    den = dense(den, 4096)\n",
    "    return den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진의 크기 224*224\n",
    "images = tf.placeholder(dtype = tf.float32, shape = [None,224,224,3])\n",
    "ima    = build_vgg16(images)\n",
    "# 입력 Text 는 필요없어서 비워둔다\n",
    "label = [['','코끼리와 오리'],['','햄스터와 물통'],['','코끼리'],['','코끼리'],['','코끼리']]\n",
    "\n",
    "def make_batch(seq_data):\n",
    "    output_batch, target_batch = [], []\n",
    "    for seq in seq_data:\n",
    "        # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙인다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        # 아래는 글자의 최대수보다 낮으면 패딩으로 채워주고 크면 최대만큼 잘라냄\n",
    "        if len(output) <= max_len:\n",
    "            for i in range(max_len - len(output)):\n",
    "                output.append(2)\n",
    "        else:\n",
    "            output = output[:max_len]\n",
    "\n",
    "        if len(target) <= max_len:\n",
    "            for i in range(max_len - len(target)):\n",
    "                target.append(2)\n",
    "        else:\n",
    "            target = target[:max_len]\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "    return  output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "n_hidden      = 4096 # CNN출력의 사이즈와 맞춘다\n",
    "total_epoch   = 201\n",
    "n_class   = n_input = dic_len\n",
    "\n",
    "# [batch size, time steps]\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "targets   = tf.placeholder(tf.int64, [None, None])\n",
    "\n",
    "# Seq2Seq 모델에서는 인코더 셀의 최종 상태값을 디코더 셀의 초기 상태값으로 넣어주지만 \n",
    "# 이 모델에서는 CNN에서 나온 출력값을 초기상태값으로 넣어준다.\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=ima,# 초기상태값\n",
    "                                            dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.layers.dense(outputs, n_class, activation=None)##단어 갯수만큼 출력을 맞춰줌\n",
    "cost  = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits = model, labels = targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess      = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "output_batch, target_batch = make_batch(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(imge):\n",
    "    seq_data = ['',''] # 처음 데이터로 시작을 뜻하는 S만들어가면 되기때문에 둘다 비워둠\n",
    "    output_batch, target_batch = make_batch([seq_data]) # S만 들어가있는 데이터\n",
    "    imge = [imge]\n",
    "    # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n",
    "    # 2번째 차원인 input 차원을 argmax 로 취해 가장 확률이 높은 글자를 예측 값으로 만든다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "    result = sess.run(prediction,\n",
    "                      feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})#여기서 S 다음에 올 글자 예측\n",
    "\n",
    "    # 원래는 디코더 입력 데이터를 다 패딩처리해서 한꺼번에 넣어주었는데\n",
    "    # 여기서는 다음단어를 예측하여 그다음 LSTM에 넣어준다\n",
    "    for i in range(max_len):\n",
    "        if result[0][i]==1: # 끝을 뜻하는 E가 번호로 1로 표현되기 때문에 1이나오면 종료\n",
    "            break\n",
    "        elif i == 0: # result에 처음 데이터는 S다음 데이터이기 때문에 다음 output으로 S를 뜻하는 0과 re 를 같이 넣어줌\n",
    "            re=result[0][i]\n",
    "            output = [0,re]\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            output = output[:2]\n",
    "            result = sess.run(prediction,\n",
    "                              feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})\n",
    "        # 계속해서 result의 출력을 아웃풋에 추가시켜 단어를 예측한다 \n",
    "        else: \n",
    "            re = result[0][i]\n",
    "            output.append(re)\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            output = output[:i+2]\n",
    "            result = sess.run(prediction,\n",
    "                              feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})\n",
    "        if i == max_len-1: # 최대길이면 그냥 출력\n",
    "            break\n",
    "\n",
    "    # 결과 값인 숫자의 인덱스에 해당하는 글자를 가져와 글자 배열을 만든다.\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "\n",
    "    # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
    "    if 'E' in decoded:\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "    else:##E가 없으면 모두출력\n",
    "        translated = ''.join(decoded)\n",
    "\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 , cost = 3.483474\n",
      "img1 -> 코끼리\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 26 , cost = 0.692378\n",
      "img1 -> 코끼리\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 51 , cost = 0.263815\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 햄스터와 물통\n",
      "img5 -> 햄스터와 물통\n",
      "Epoch 76 , cost = 0.188956\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 햄스터와 물통\n",
      "Epoch 101 , cost = 0.030983\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 126 , cost = 0.001756\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 151 , cost = 0.000451\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 176 , cost = 0.000453\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 201 , cost = 0.000305\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 226 , cost = 0.000219\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 251 , cost = 0.000399\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 276 , cost = 0.000158\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 301 , cost = 0.000102\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 326 , cost = 0.000153\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 351 , cost = 0.000120\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 376 , cost = 0.000081\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 401 , cost = 0.000121\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 426 , cost = 0.000120\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 451 , cost = 0.000081\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 476 , cost = 0.000063\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 501 , cost = 0.000054\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 526 , cost = 0.000066\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 551 , cost = 0.000050\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 576 , cost = 0.000066\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 601 , cost = 0.000065\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 626 , cost = 0.000058\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 651 , cost = 0.000061\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 676 , cost = 0.000054\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 701 , cost = 0.000055\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 726 , cost = 0.000032\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 751 , cost = 0.000031\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 776 , cost = 0.000047\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 801 , cost = 0.000058\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 826 , cost = 0.000026\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 851 , cost = 0.000030\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 876 , cost = 0.000031\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 901 , cost = 0.000037\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 926 , cost = 0.000030\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 951 , cost = 0.000043\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 976 , cost = 0.000019\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n",
      "Epoch 1001 , cost = 0.000026\n",
      "img1 -> 햄스터와 물통\n",
      "img2 -> 코끼리\n",
      "img5 -> 코끼리\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd5885172e8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cost_data = []\n",
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={images :data,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    cost_data.append(loss)\n",
    "    if epoch % 25 == 0: # 훈련 잘되었는지 10번마다 확인\n",
    "        print('Epoch {} , cost = {:.6f}'.format(epoch + 1, loss + 0))\n",
    "        print('img1 ->', translate(img1))\n",
    "        print('img2 ->', translate(img2))\n",
    "        print('img5 ->', translate(img5))\n",
    "\n",
    "import pandas as pd\n",
    "cost_data = pd.Series(cost_data)\n",
    "cost_data.plot(figsize=(15,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **3 Attention 보완모델**\n",
    "어텐션.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageio import imread, imsave\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "img  = imread('./data/코끼리_tinted.jpg')\n",
    "img1 = imread('./data/햄스터_tinted.jpg')\n",
    "img2 = imread('./data/코끼리11.jpg')\n",
    "img3 = imread('./data/코끼리22.jpg')\n",
    "img4 = imread('./data/코끼리33.jpg')\n",
    "img5 = imread('./data/코끼리44.jpg')\n",
    "data = [img, img1,img2,img3,img4]\n",
    "data = np.array(data)\n",
    "char_arr = [c for c in 'SEP가아나기다코구끼라리와마오리바햄스사터뒤에물통 ']\n",
    "num_dic  = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len  = len(num_dic)+1\n",
    "max_len  = 10 #입력과 출력의 최대 글자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "def attend(contexts, output):\n",
    "    reshaped_contexts = tf.reshape(contexts, [-1, 256])\n",
    "    # use 1 fc layer to attend\n",
    "    logits1 =tf.layers.dense(reshaped_contexts,\n",
    "                             units = 1,\n",
    "                             activation = None,\n",
    "                             use_bias = False,\n",
    "                             name = 'fc_a')\n",
    "    print(logits1)\n",
    "    logits1 = tf.reshape(logits1, [-1, 196])\n",
    "    logits2 = tf.layers.dense(output,\n",
    "                              units = 196,\n",
    "                              activation = None,\n",
    "                              use_bias = False,\n",
    "                              name = 'fc_b')\n",
    "    a = tf.shape(logits1)\n",
    "    b = tf.shape(logits2)\n",
    "    logit = logits1 + logits2\n",
    "    logit = tf.nn.softmax(logit)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inputs, filters, kernel_size=(3, 3), strides=(1, 1),\n",
    "           activation=tf.nn.relu, use_bias=True, name=None):\n",
    "    return tf.layers.conv2d(inputs=inputs, filters=filters,\n",
    "                            kernel_size=kernel_size, strides=strides,\n",
    "                            padding='same', activation=activation,\n",
    "                            use_bias=use_bias, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D Max Pooling layer.\n",
    "def max_pool2d(inputs, pool_size=(2, 2), strides=(2, 2), name=None):\n",
    "    return tf.layers.max_pooling2d(inputs=inputs, pool_size=pool_size, \n",
    "                                   strides=strides, padding='same', name=name)\n",
    "\n",
    "# Fully-connected layer\n",
    "def dense(inputs, units, activation=tf.tanh, use_bias=True, name=None):\n",
    "    return tf.layers.dense(inputs=inputs, units=units, \n",
    "                           activation=activation, use_bias=use_bias, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the VGG16 net.\n",
    "# 사진의 크기 224*224\n",
    "# 5th Layer 에서 14*14*256 으로 출력\n",
    "def build_vgg16(images):\n",
    "    conv1_1_feats = conv2d(images, 64, name='conv1_1')\n",
    "    conv1_2_feats = conv2d(conv1_1_feats, 64, name='conv1_2')\n",
    "    pool1_feats   = max_pool2d(conv1_2_feats, name='pool1')\n",
    "    \n",
    "    conv2_1_feats = conv2d(pool1_feats, 128, name='conv2_1')\n",
    "    conv2_2_feats = conv2d(conv2_1_feats, 128, name='conv2_2')\n",
    "    pool2_feats   = max_pool2d(conv2_2_feats, name='pool2')\n",
    "    \n",
    "    conv3_1_feats = conv2d(pool2_feats, 256, name='conv3_1')\n",
    "    conv3_2_feats = conv2d(conv3_1_feats, 256, name='conv3_2')\n",
    "    conv3_3_feats = conv2d(conv3_2_feats, 256, name='conv3_3')\n",
    "    pool3_feats = max_pool2d(conv3_3_feats, name='pool3')\n",
    "\n",
    "    conv4_1_feats = conv2d(pool3_feats, 256, name='conv4_1')\n",
    "    conv4_2_feats = conv2d(conv4_1_feats, 256, name='conv4_2')\n",
    "    conv4_3_feats = conv2d(conv4_2_feats, 256, name='conv4_3')\n",
    "    pool4_feats = max_pool2d(conv4_3_feats, name='pool4')\n",
    "\n",
    "    conv5_1_feats = conv2d(pool4_feats, 256, name='conv5_1')\n",
    "    conv5_2_feats = conv2d(conv5_1_feats, 256, name='conv5_2')\n",
    "    conv5_3_feats = conv2d(conv5_2_feats, 256, name='conv5_3') \n",
    "\n",
    "    reshaped_conv5_3_feats = tf.reshape(conv5_3_feats, [-1, 196, 256])\n",
    "    return reshaped_conv5_3_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    output_batch, target_batch = [], []\n",
    "    for seq in seq_data:\n",
    "        # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙인다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        # 아래는 글자의 최대수보다 낮으면 패딩으로 채워주고 크면 최대만큼 잘라냄\n",
    "        if len(output) <= max_len:\n",
    "            for i in range(max_len - len(output)):\n",
    "                output.append(2)\n",
    "        else: output = output[:max_len]\n",
    "\n",
    "        if len(target) <= max_len:\n",
    "            for i in range(max_len - len(target)):\n",
    "                target.append(2)\n",
    "        else: target = target[:max_len]\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "    return  output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c9029df50583>\", line 7, in conv2d\n    use_bias = use_bias, name = name)\n  File \"<ipython-input-4-a7d3ed2d735e>\", line 3, in build_vgg16\n    conv1_1_feats = conv2d(images,        64,  name='conv1_1')\n  File \"<ipython-input-5-ae19bf044654>\", line 3, in <module>\n    ima    = build_vgg16(images)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-227379179e49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mima\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mbuild_vgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mlabel\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리와 오리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'햄스터와 물통'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# 입력데이터는 필요없기에 비워둠.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-cdad9f868da2>\u001b[0m in \u001b[0;36mbuild_vgg16\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# 5th Layer 에서 14*14*256 으로 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mconv1_1_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mconv1_2_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_1_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpool1_feats\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_2_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-91c02413d3c0>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, activation, use_bias, name)\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                             use_bias=use_bias, name=name)\n\u001b[0m",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m       _scope=name)\n\u001b[0;32m--> 619\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \"\"\"\n\u001b[0;32m--> 825\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m               \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    142\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                     dtype=self.dtype)\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    413\u001b[0m     new_variable = getter(\n\u001b[1;32m    414\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c9029df50583>\", line 7, in conv2d\n    use_bias = use_bias, name = name)\n  File \"<ipython-input-4-a7d3ed2d735e>\", line 3, in build_vgg16\n    conv1_1_feats = conv2d(images,        64,  name='conv1_1')\n  File \"<ipython-input-5-ae19bf044654>\", line 3, in <module>\n    ima    = build_vgg16(images)\n"
     ]
    }
   ],
   "source": [
    "data        = np.array(data)\n",
    "images      = tf.placeholder(dtype=tf.float32, shape=[None,224,224,3])\n",
    "ima         = build_vgg16(images)\n",
    "label       = [['','코끼리와 오리'],['','햄스터와 물통'],['','코끼리'],['','코끼리'],['','코끼리']] # 입력데이터는 필요없기에 비워둠.\n",
    "learning_rate = 0.0001\n",
    "n_hidden    = 4096 #CNN출력의 사이즈와 맞춰줌\n",
    "total_epoch = 1001\n",
    "n_class     = n_input = dic_len\n",
    "dec_input   = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "# [batch size, time steps]\n",
    "targets     = tf.placeholder(tf.int64, [None, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seq2seq 의 Decoder 실행 \n",
    "# Seq2Seq 모델에서는 인코더 셀의 최종 상태값을 디코더 셀의 초기 상태값으로 넣어주지만 이 모델에서는 CNN에서 나온 출력값을 초기상태값으로 넣어준다\n",
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "    outputs, dec_states = tf.nn.dynamic_rnn(dec_cell, dec_input,\n",
    "                                            initial_state=ima, # 초기상태값\n",
    "                                            dtype=tf.float32)\n",
    "\n",
    "model = tf.layers.dense(outputs, n_class, activation=None) # 단어 갯수만큼 출력을 맞춰줌\n",
    "cost  = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess  = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "output_batch, target_batch = make_batch(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음 데이터로 시작을 뜻하는 S를 만들어가면 되기때문에 둘 다 비워둔다 \n",
    "def translate(imge):\n",
    "    seq_data = ['','']\n",
    "    output_batch, target_batch = make_batch([seq_data])##S만 들어가있는 데이터\n",
    "    imge = [imge]\n",
    "    # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n",
    "    # 2번째 차원인 input 차원을 argmax 로 취해 가장 확률이 높은 글자를 예측 값으로 만든다.\n",
    "    prediction = tf.argmax(model, 2)\n",
    "    #여기서 S 다음에 올 글자 예측\n",
    "    result = sess.run(prediction, feed_dict={images :imge, dec_input: output_batch})\n",
    "    # 원래는 디코더 입력 데이터를 다 패딩처리해서 한꺼번에 넣어주었는데\n",
    "    # 여기서는 다음단어를 예측하여 그다음 LSTM에 넣어줌\n",
    "    for i in range(max_len):\n",
    "        if result[0][i]==1: # 끝을 뜻하는 E가 번호로 1로 표현되기 때문에 1이나오면 종료\n",
    "            break\n",
    "        # result에 처음 데이터는 S다음 데이터이기 때문에 다음 output으로 S를 뜻하는 0과 re 를 같이 넣어줌\n",
    "        elif i == 0: \n",
    "            re     = result[0][i]\n",
    "            output = [0,re]\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            output = output[:2]\n",
    "            result = sess.run(prediction, feed_dict={images :imge, dec_input: output_batch})\n",
    "        # 계속해서 result의 출력을 아웃풋에 추가시켜 단어를 예측한다\n",
    "        else: \n",
    "            re = result[0][i]\n",
    "            output.append(re)\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            output = output[:i+2]\n",
    "            result = sess.run(prediction, feed_dict={images :imge, dec_input: output_batch})\n",
    "        if i == max_len - 1: # 최대길이면 그냥 출력\n",
    "            break\n",
    "\n",
    "    # 결과 값인 숫자의 인덱스에 해당하는 글자를 가져와 글자 배열을 만든다.\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
    "    if 'E' in decoded:\n",
    "        end = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "    else: translated = ''.join(decoded) # E가 없으면 모두출력\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(total_epoch):\n",
    "    _, loss = sess.run([optimizer, cost],\n",
    "                       feed_dict={images :data,\n",
    "                                  dec_input: output_batch,\n",
    "                                  targets: target_batch})\n",
    "    print('Epoch:', '%04d' % (epoch + 1),\n",
    "          'cost =', '{:.6f}'.format(loss))\n",
    "    if epoch % 10 == 0: # 훈련잘되었는지 10번마다 확인\n",
    "        print('img ->', translate(img2))\n",
    "        print('img ->', translate(img1))\n",
    "        print('img ->', translate(img5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "## **4 확인모델**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from imageio import imread, imsave\n",
    "img  = imread('./data/코끼리_tinted.jpg')\n",
    "img1 = imread('./data/햄스터_tinted.jpg')\n",
    "img2 = imread('./data/코끼리11.jpg')\n",
    "img3 = imread('./data/코끼리22.jpg')\n",
    "img4 = imread('./data/코끼리33.jpg')\n",
    "img5 = imread('./data/코끼리44.jpg')\n",
    "data = [img, img1,img2,img3,img4]\n",
    "data = np.array(data)\n",
    "char_arr = [c for c in 'SEP가아나기다코구끼라리와마오리바햄스사터뒤에물통 ']\n",
    "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
    "dic_len = len(num_dic)+1\n",
    "max_len = 10 #입력과 출력의 최대 글자 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Mechanism\n",
    "def attend(contexts, output):\n",
    "    reshaped_contexts = tf.reshape(contexts, [-1, 256])\n",
    "    # use 1 fc layer to attend\n",
    "    logits1 =tf.layers.dense(reshaped_contexts, units=1, \n",
    "                             activation=None, use_bias=False, \n",
    "                             name='fc_a', reuse=tf.AUTO_REUSE)\n",
    "    logits1 = tf.reshape(logits1, [-1, 196])\n",
    "    logits2 = tf.layers.dense(output,units = 196,\n",
    "                              activation = None, use_bias=False,\n",
    "                              name = 'fc_b',reuse = tf.AUTO_REUSE)\n",
    "    logit = logits1 + logits2\n",
    "    logit = tf.nn.softmax(logit)\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(inputs, filters, kernel_size=(3, 3), strides=(1, 1),\n",
    "           activation = tf.nn.relu, use_bias=True, name=None):\n",
    "    return tf.layers.conv2d(inputs = inputs, filters = filters, \n",
    "                            kernel_size = kernel_size, strides = strides,\n",
    "                            padding = 'same', activation = activation,\n",
    "                            use_bias = use_bias, name = name)\n",
    "# \"\"\" 2D Max Pooling layer. \"\"\"\n",
    "def max_pool2d(inputs, pool_size=(2, 2), strides=(2, 2), name=None):  \n",
    "    return tf.layers.max_pooling2d(inputs=inputs, pool_size=pool_size,\n",
    "                                   strides=strides, padding='same', name=name)\n",
    "# Fully-connected layer.\n",
    "def dense(inputs, units, activation=tf.tanh, use_bias=True, name=None):\n",
    "    return tf.layers.dense(inputs=inputs, units=units, activation=activation,\n",
    "                           use_bias=use_bias, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진의 크기 224*224\n",
    "# Build the VGG16 net.\n",
    "def build_vgg16(images):\n",
    "    conv1_1_feats = conv2d(images, 64, name='conv1_1')\n",
    "    conv1_2_feats = conv2d(conv1_1_feats, 64, name='conv1_2')\n",
    "    pool1_feats = max_pool2d(conv1_2_feats, name='pool1')\n",
    "\n",
    "    conv2_1_feats =conv2d(pool1_feats, 128, name='conv2_1')\n",
    "    conv2_2_feats = conv2d(conv2_1_feats, 128, name='conv2_2')\n",
    "    pool2_feats = max_pool2d(conv2_2_feats, name='pool2')\n",
    "\n",
    "    conv3_1_feats = conv2d(pool2_feats, 256, name='conv3_1')\n",
    "    conv3_2_feats = conv2d(conv3_1_feats, 256, name='conv3_2')\n",
    "    conv3_3_feats = conv2d(conv3_2_feats, 256, name='conv3_3')\n",
    "    pool3_feats = max_pool2d(conv3_3_feats, name='pool3')\n",
    "\n",
    "    conv4_1_feats = conv2d(pool3_feats, 256, name='conv4_1')\n",
    "    conv4_2_feats = conv2d(conv4_1_feats, 256, name='conv4_2')\n",
    "    conv4_3_feats = conv2d(conv4_2_feats, 256, name='conv4_3')\n",
    "    pool4_feats = max_pool2d(conv4_3_feats, name='pool4')\n",
    "\n",
    "    conv5_1_feats = conv2d(pool4_feats, 256, name='conv5_1')\n",
    "    conv5_2_feats = conv2d(conv5_1_feats, 256, name='conv5_2')\n",
    "    conv5_3_feats = conv2d(conv5_2_feats, 256, name='conv5_3')#14*14*256 으로 출력\n",
    "    reshaped_conv5_3_feats = tf.reshape(conv5_3_feats, [-1, 196, 256])\n",
    "    context_mean = tf.reduce_mean(reshaped_conv5_3_feats, axis=1)\n",
    "    memory = dense(context_mean, units=4096, activation=None, name='fc_a')\n",
    "    output = dense(context_mean, units=4096, activation=None, name='fc_b')\n",
    "    return reshaped_conv5_3_feats, memory, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(seq_data):\n",
    "    output_batch, target_batch = [], []\n",
    "    for seq in seq_data:\n",
    "        # 디코더 셀의 입력값. 시작을 나타내는 S 심볼을 맨 앞에 붙여준다.\n",
    "        output = [num_dic[n] for n in ('S' + seq[1])]\n",
    "        # 학습을 위해 비교할 디코더 셀의 출력값. 끝나는 것을 알려주기 위해 마지막에 E 를 붙인다.\n",
    "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
    "        ##아래는 글자의 최대수보다 낮으면 패딩으로 채워주고 크면 최대만큼 잘라냄\n",
    "        if len(output) <= max_len:\n",
    "            for i in range(max_len - len(output)):\n",
    "                output.append(2)\n",
    "        else:\n",
    "            output = output[:max_len]\n",
    "\n",
    "        if len(target) <= max_len:\n",
    "            for i in range(max_len - len(target)):\n",
    "                target.append(2)\n",
    "        else:\n",
    "            target = target[:max_len]\n",
    "        output_batch.append(np.eye(dic_len)[output])\n",
    "        target_batch.append(target)\n",
    "    return  output_batch, target_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable conv1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c9029df50583>\", line 7, in conv2d\n    use_bias = use_bias, name = name)\n  File \"<ipython-input-4-a7d3ed2d735e>\", line 3, in build_vgg16\n    conv1_1_feats = conv2d(images,        64,  name='conv1_1')\n  File \"<ipython-input-5-ae19bf044654>\", line 3, in <module>\n    ima    = build_vgg16(images)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-d1b4f2f2ab62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mima\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리와 오리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'햄스터와 물통'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'코끼리'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m##입력데이터는 필요없기에 비워둠.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m \u001b[0;31m#CNN출력의 사이즈와 맞춰줌\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-af70c2308dc9>\u001b[0m in \u001b[0;36mbuild_vgg16\u001b[0;34m(images)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Build the VGG16 net.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_vgg16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mconv1_1_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mconv1_2_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_1_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'conv1_2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpool1_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconv1_2_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pool1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-8a921599a45b>\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, activation, use_bias, name)\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                             use_bias = use_bias, name = name)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# \"\"\" 2D Max Pooling layer. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m       _scope=name)\n\u001b[0;32m--> 619\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \"\"\"\n\u001b[0;32m--> 825\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    694\u001b[0m               \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m           \u001b[0;31m# Note: not all sub-classes of Layer call Layer.__init__ (especially\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    142\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                     dtype=self.dtype)\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             partitioner=partitioner)\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/training/checkpointable.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    413\u001b[0m     new_variable = getter(\n\u001b[1;32m    414\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    437\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Jupyter/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    745\u001b[0m                          \u001b[0;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[0;32m--> 747\u001b[0;31m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[1;32m    748\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable conv1_1/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"<ipython-input-3-c9029df50583>\", line 7, in conv2d\n    use_bias = use_bias, name = name)\n  File \"<ipython-input-4-a7d3ed2d735e>\", line 3, in build_vgg16\n    conv1_1_feats = conv2d(images,        64,  name='conv1_1')\n  File \"<ipython-input-5-ae19bf044654>\", line 3, in <module>\n    ima    = build_vgg16(images)\n"
     ]
    }
   ],
   "source": [
    "images = tf.placeholder( dtype=tf.float32, shape=[None,224,224,3])\n",
    "ima, state, out = build_vgg16(images)\n",
    "label = [['','코끼리와 오리'],['','햄스터와 물통'],['','코끼리'],['','코끼리'],['','코끼리']]##입력데이터는 필요없기에 비워둠.\n",
    "learning_rate = 0.0001\n",
    "n_hidden = 4096 #CNN출력의 사이즈와 맞춰줌\n",
    "total_epoch = 1001\n",
    "\n",
    "n_class =dic_len\n",
    "n_input =dic_len\n",
    "output_batch, target_batch = make_batch(label)\n",
    "\n",
    "\n",
    "dec_input = tf.placeholder(tf.float32, [None, None, n_input])\n",
    "# [batch size, time steps]\n",
    "\n",
    "targets = tf.placeholder(tf.int64, [None, None])\n",
    "output_batch = np.array(output_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope('decode'):\n",
    "    dec_cell = tf.nn.rnn_cell.BasicRNNCell(n_hidden)\n",
    "    dec_cell = tf.nn.rnn_cell.DropoutWrapper(dec_cell, output_keep_prob=0.5)\n",
    "\n",
    "    '''Seq2Seq 모델에서는 인코더 셀의 최종 상태값을 디코더 셀의 초기 상태값으로 넣어주지만 이 모델에서는 CNN에서 나온 출력값을 초기상태값으로 넣어줌.'''\n",
    "\n",
    "    predictions = []\n",
    "    sente = []\n",
    "    al = []\n",
    "    for i in range(max_len):\n",
    "        if i == 0:\n",
    "            dec_inputa = dec_input[:,i,:]\n",
    "            alpha=attend(ima,out)\n",
    "            context = tf.reduce_sum(ima * tf.expand_dims(alpha, 2), axis=1)\n",
    "            a = tf.nn.softmax(alpha)\n",
    "            a = tf.reshape(a, [14, 14])\n",
    "            al.append(a)\n",
    "            current_input = tf.concat([context, dec_inputa], 1)\n",
    "            outputs, dec_states = dec_cell(current_input, state)\n",
    "            expanded_output = tf.concat([outputs, context, dec_inputa], axis=1)\n",
    "            model = tf.layers.dense(expanded_output, n_class, activation=None)  ##단어 갯수만큼 출력을 맞춰줌\n",
    "            probs = tf.nn.softmax(model)\n",
    "            prediction = tf.argmax(model, 1)\n",
    "            sente.append(model)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "        else:\n",
    "            dec_inputa = dec_input[:, i, :]\n",
    "            alpha = attend(ima, outputs)\n",
    "            context = tf.reduce_sum(ima * tf.expand_dims(alpha, 2), axis=1)\n",
    "\n",
    "            a = tf.nn.softmax(alpha)\n",
    "            a = tf.reshape(a, [14, 14])\n",
    "            al.append(a)\n",
    "\n",
    "            current_input = tf.concat([context, dec_inputa], 1)\n",
    "            outputs, dec_states = dec_cell(current_input, dec_states)\n",
    "            expanded_output = tf.concat([outputs, context, dec_inputa], axis=1)\n",
    "            model = tf.layers.dense(expanded_output, n_class, activation=None)##단어 갯수만큼 출력을 맞춰줌\n",
    "            probs = tf.nn.softmax(model)\n",
    "\n",
    "            prediction = tf.argmax(model, 1)\n",
    "            sente.append(model)\n",
    "            predictions.append(prediction)\n",
    "\n",
    "    predictions = tf.transpose(predictions)\n",
    "    sente = tf.transpose(sente,(1,0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(imge):\n",
    "    seq_data = ['','']#처음 데이터로 시작을 뜻하는 S만들어가면 되기때문에 둘다 비워둠\n",
    "    output_batch, target_batch = make_batch([seq_data])##S만 들어가있는 데이터\n",
    "    imge = [imge]\n",
    "    # 결과가 [batch size, time step, input] 으로 나오기 때문에,\n",
    "    # 2번째 차원인 input 차원을 argmax 로 취해 가장 확률이 높은 글자를 예측 값으로 만든다.\n",
    "    result = sess.run(predictions,\n",
    "                      feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})#여기서 S 다음에 올 글자 예측\n",
    "\n",
    "    # 원래는 디코더 입력 데이터를 다 패딩처리해서 한꺼번에 넣어주었는데\n",
    "    # 여기서는 다음단어를 예측하여 그다음 LSTM에 넣어줌\n",
    "    for i in range(max_len):\n",
    "        if result[0][i]==1: # 끝을 뜻하는 E가 번호로 1로 표현되기 때문에 1이나오면 종료\n",
    "            break\n",
    "        elif i == 0:        # result에 처음 데이터는 S다음 데이터이기 때문에 다음 output으로 S를 뜻하는 0과 re 를 같이 넣어줌\n",
    "            re=result[0][i]\n",
    "            output = [0,re]\n",
    "            pad = [2] * (8)\n",
    "            output = output + pad\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            result = sess.run(predictions,\n",
    "                              feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})\n",
    "        else: # 계속해서 result의 출력을 아웃풋에 추가시켜 단어를 예측\n",
    "            re = result[0][i]\n",
    "            output.append(re)\n",
    "            pad = [2] * (max_len - (i + 2))\n",
    "            output       = output + pad\n",
    "            output_batch = []\n",
    "            output_batch.append(np.eye(dic_len)[output])\n",
    "            result = sess.run(predictions,\n",
    "                              feed_dict={images :imge,\n",
    "                                 dec_input: output_batch})\n",
    "            alq = sess.run(al,feed_dict={images: imge,\n",
    "                                         dec_input: output_batch})\n",
    "        if i == max_len-1: # 최대길이면 그냥 출력\n",
    "            break\n",
    "            \n",
    "    # 결과 값인 숫자의 인덱스에 해당하는 글자를 가져와 글자 배열을 만든다.\n",
    "    decoded = [char_arr[i] for i in result[0]]\n",
    "    # 출력의 끝을 의미하는 'E' 이후의 글자들을 제거하고 문자열로 만든다.\n",
    "    if 'E' in decoded:\n",
    "        end        = decoded.index('E')\n",
    "        translated = ''.join(decoded[:end])\n",
    "    else: # E가 없으면 모두출력\n",
    "        translated = ''.join(decoded)\n",
    "    return translated, alq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(\n",
    "            tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=sente, labels=targets))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "save_path = \"./data/model.ckpt\"\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,save_path=save_path)\n",
    "c,v   = translate(img2)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "dx, dy = 0.001, 0.001\n",
    "x = np.arange(-7, 7, dx)\n",
    "y = np.arange(-7, 7, dy)\n",
    "xmin, xmax, ymin, ymax = np.amin(x), np.amax(x), np.amin(y), np.amax(y)\n",
    "extent = xmin, xmax, ymin, ymax\n",
    "fig = plt.figure(frameon=False)\n",
    "im1 = plt.imshow(v[2], cmap=plt.cm.gray, interpolation='nearest', extent=extent)\n",
    "print(v)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
